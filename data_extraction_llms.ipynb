{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --quiet langchain langchain-community langchain-openai chromadb\n",
    "!pip3 install --upgrade --quiet pypdf pandas streamlit python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OPENAI_API_KEY=os.environ.get(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "llm.invoke(\"Tell me a joke about cats please\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process the PDF Document\n",
    "loader = PyPDFLoader('data/Annie Jacobsen - Operation Paperclip. The Secret Intelligence Program that Brought Nazi Scientists to America - 2014.pdf')\n",
    "pages= loader.load()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split Document\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,\n",
    "                                               chunk_overlap=200,\n",
    "                                               length_function=len,\n",
    "                                               separators= [\"\\n\\n\", '\\n', \" \"])\n",
    "text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Create Embeddings\n",
    "\n",
    "def get_embedding_function():\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model= \"text-embedding-ada-002\", openai_api_key=OPENAI_API_KEY\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding_function = get_embedding_function()\n",
    "test_vector= embedding_function.embed_query('cat')\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(evaluator=\"embedding_distance\",\n",
    "                           embeddings = embedding_function)\n",
    "\n",
    "evaluator.evaluate_strings(prediction = \"Nazi\", reference = \"German\")\n",
    "evaluator.evaluate_strings(prediction = \"Nazi\", reference = \"American\")\n",
    "## Create A Vector database\n",
    "\n",
    "import uuid\n",
    "\n",
    "def create_vectorstore(chunks, embedding_function, vectorstore_path):\n",
    "    #create a list if unique ids for each document bsed on the content\n",
    "    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in docs]\n",
    "\n",
    "    #ensure that only unique docs with unique ids are kept\n",
    "    unique_ids = set()\n",
    "    unique_chunks = []\n",
    "\n",
    "    unique_chunks = []\n",
    "    for chunk, id in zip(chunks.ids):\n",
    "         if id not in unique_ids:\n",
    "              unique_ids.add(id)\n",
    "              unique_chunks.append(chunk)\n",
    "\n",
    "    \n",
    "    # Create a New Chroma Database for the documents\n",
    "    vectorstore = Chroma.from_documents(documents=unique_chunks,\n",
    "                                        ids = list(unique_ids),\n",
    "                                        embedding=embedding_function,\n",
    "                                        persist_directory=vectorstore_path)\n",
    "    \n",
    "    vectorstore.persist()\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "#create a vecotr store\n",
    "vectorstore = create_vectorstore(chunks = chunks,\n",
    "                                  embedding_function = embedding_function,\n",
    "                                  vectorstore_path=\"vectorstore_chroma\")\n",
    "\n",
    "# Load vectorestore\n",
    "vectorstore= Chroma(persist_directory=\"vectorstore_chroma\", embedding_function=embedding_function)\n",
    "\n",
    "# Create retirever and get relevant chunks\n",
    "retriever = vectorstore.as_retriever(search_type='similarity')\n",
    "relevant_chunks= retriever.invoke('What is the title of the book?')\n",
    "relevant_chunks\n",
    "# Query for relavant data\n",
    "#load vectorstore \n",
    "vectorstore = Chroma(persist_directoy = \"vectorstore_chroma\", embedding_function=embedding_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
